{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/skojaku/Practical-Guide-to-Sentence-Transformers/blob/main/notebook/Practical_Guide_to_Sentence_Transformers.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7Vl68CX6mwh"
      },
      "source": [
        "# **Tutorial: Practical Guide to Sentence Transformers**\n",
        "- v 0.1: _Monday, 17th Mar 2025_ - Added pipeline exploration\n",
        "- v 0.0: _Monday, 27th Sep 2021_ - Added sentence-transformer models and interactive hands-on\n",
        "\n",
        "By Sadamori Kojaku.\n",
        "\n",
        "## References\n",
        "- *Paper*:\n",
        "Reimers, Nils, and Iryna Gurevych. 2019. “Sentence-BERT: Sentence Embeddings \n",
        "Using Siamese BERT-Networks.” EMNLP. arXiv [cs.CL]. arXiv. http://arxiv.org/abs/1908.10084.\n",
        "\n",
        "\n",
        "- *Library*:\n",
        "https://www.sbert.net/\n",
        "\n",
        "\n",
        "- *Video*:\n",
        "https://www.youtube.com/watch?v=Ey81KfQ3PQU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7nHP_Uv7Ugy"
      },
      "source": [
        "# **1. How to use sentence-transformer models**\n",
        "\n",
        "## **1.1. Setup**\n",
        "\n",
        "First, we need the following libraries to use the sentence BERT. \n",
        "\n",
        "- [transformers](https://huggingface.co/transformers/) provides a variety of pre-trained transformer-based models.\n",
        "- [sentence-transformer](https://www.sbert.net/index.html) provides a lightweight wrapper for transformers and training procedures.\n",
        "\n",
        "If you are running this notebook locally, the easiest way is to use [uv](https://github.com/astral-sh/uv) to install the libraries.  \n",
        "\n",
        "```bash \n",
        "pip install uv # Install uv \n",
        "uv venv .venv # Create a virtual environment\n",
        "source .venv/bin/activate # activate the virtual environment\n",
        "uv pip install torch torchvision torchaudio # Install torch\n",
        "uv pip install sentence-transformers transformers datasets jupyter # Install other libraries\n",
        "```\n",
        "\n",
        "For colab users, comment out the following cell and run it to install the libraries.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "neBL0sqqceM9"
      },
      "outputs": [],
      "source": [
        "#%%capture\n",
        "#!pip install -U sentence-transformers datasets transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ANVzo3FL9gJd"
      },
      "source": [
        "After installing the libraries, we import the modules for loading sentence transformers."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "ZOfeGp2Kcn7A"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GioGzK4t8EUo"
      },
      "source": [
        "## **1.2. Model**\n",
        "\n",
        "Next, we need to select the transformer-based model for embedding. There are more than 15,000 models, and what model to use is a critical modeling decision.\n",
        "\n",
        "The key feature of sentence-transformers is fine-tuning, i.e., they are trained such that ***sentence embeddings*** are useful, whereas pre-trained models are trained such that ***word embeddings*** are useful. The fine-tuned models can be downloaded from [sentence-transformers library](https://www.sbert.net/docs/pretrained_models.html). You can also use pre-trained models in [Hugging Face model hub](https://huggingface.co/models).\n",
        "\n",
        "As a demonstration of sentence-transformers, we use a fine-tuned model. The model is trained on various sentence pairs in Wikipedia, scientific papers, reviews, and Q&A websites. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "_UthTu4Mc9dp"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"paraphrase-MiniLM-L6-v2\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ph6xR526_HUD"
      },
      "source": [
        "The model can be downloaded by"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FJ6dZp_-dIT_"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer(MODEL_NAME)\n",
        "# model = SentenceTransformer(MODEL_NAME, device = 0) # device = -1 == CPU, device = 0 == GPU"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n0PR6IBd_WvH"
      },
      "source": [
        "This method takes a list of sentences and produces an array of embedding (`numpy.ndarray`). Each row in the array is the embedding vector for a given sentence."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xqsSK_hbdK2a"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"This framework generates embeddings for each input sentence\",\n",
        "    \"Sentences are passed as a list of string.\",\n",
        "    \"The quick brown fox jumps over the lazy dog.\",\n",
        "]\n",
        "\n",
        "# Sentences are encoded by calling model.encode()\n",
        "embeddings = model.encode(sentences)\n",
        "\n",
        "# Print the embeddings\n",
        "for i, sentence in enumerate(sentences):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding[:10]:\", embeddings[i, :10])\n",
        "    print(\"\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9LUjzydahwW"
      },
      "source": [
        "## **1.3. Semantic search**\n",
        "\n",
        "A key feature of BERT is its ability to capture semantics. To demonstrate this, let us consider a basic NLP task: \n",
        "- You are given pairs of sentences, e.g., \"He likes eating noodles\", and \"His favorite food is noodles\".\n",
        "- You are asked to provide semantic relatedness of the given sentences. \n",
        "\n",
        "To calculate the semantic relatedness, we'll embed the given sentences and calculate the similarity.\n",
        "\n",
        "Let us consider the following sentence pairs in order of semantic relatedness. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "HI5HDrbwdG-V"
      },
      "outputs": [],
      "source": [
        "sentence_pairs = [\n",
        "    [\n",
        "        \"The little bird is bathing in the sink.\",\n",
        "        \"Birdie is washing itself in the water basin.\",\n",
        "    ],\n",
        "    [\n",
        "        \"Two boys on a couch are playing video games.\",\n",
        "        \"Two boys are playing a video game.\",\n",
        "    ],\n",
        "    [\n",
        "        \"John said he is considered a witness but not suspect.\",\n",
        "        \"'He is not a suspect anymore', John said.\",\n",
        "    ],\n",
        "    [\"They flew out of the nest in groups.\", \"They flew into the nest together.\"],\n",
        "    [\n",
        "        \"The woman is playing the violin.\",\n",
        "        \"The young lady enjoys listening to the guitar.\",\n",
        "    ],\n",
        "    [\n",
        "        \"The black dog is running through the snow.\",\n",
        "        \"A race car driver is driving his car through the mud.\",\n",
        "    ],\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8_wXgzSHdb9I"
      },
      "source": [
        "The first sentence is semantically equivalent although no word except 'is' and 'in' are in common (and thus a very challenging example). The second sentence pair is also semantically very similar but some details are different. The last sentence pair is semantically different.\n",
        "\n",
        "Can the sentence-transformers really capture the semantic relatedness?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OaAdkPWrdN_K"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "MODEL_NAME = \"paraphrase-MiniLM-L6-v2\"\n",
        "\n",
        "model = SentenceTransformer(MODEL_NAME)\n",
        "\n",
        "\n",
        "def cosine_similarity_matrix(emb):\n",
        "    emb = np.einsum(\"ij,i->ij\", emb, 1 / np.linalg.norm(emb, axis=1))\n",
        "    return emb @ emb.T\n",
        "\n",
        "\n",
        "for sentence_pair in sentence_pairs:\n",
        "    emb = model.encode(sentence_pair)\n",
        "    sim = cosine_similarity_matrix(emb)[0, 1]\n",
        "    print(\n",
        "        \"sim = {sim:.2f}: '{sent1}' '{sent2}'\".format(\n",
        "            sent1=sentence_pair[0], sent2=sentence_pair[1], sim=sim\n",
        "        )\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbQUkoYuemNF"
      },
      "source": [
        "The similarity for the first sentence is relatively high even though only two general words ('in' and 'is') are in common. The second to the fourth sentences have clearly higher similarity than those of semantically less related sentence pairs (the fifth and sixth). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PphSc5Mc6kB1"
      },
      "source": [
        "## **1.4. Semantic search with pre-trained models**\n",
        "\n",
        "The `sentence-transformers` makes it easy for you to generate sentence embeddings with pre-trained models. \n",
        "Although pre-trained models are not trained for sentence embeddings, they would capture some aspects of semantic relatedness of words. With pre-trained models, the embedding for a sentence is calculated by the average of the embeddings of words in the sentence.\n",
        "\n",
        "Pre-trained models are sometimes useful because there are more than 15,000 pre-trained models trained for various tasks, whereas there are only less than 50 sentence-transformer models trained for some specific tasks. \n",
        "\n",
        "An example is sentiment analysis: given a sentence, decide whether or not the sentiment is positive or negative. As of 09/23/2021, there is no sentence-transformers model but numerous pre-trained models for sentiment analysis.\n",
        "\n",
        "Here, we use a model in [hugging models hub](https://huggingface.co/) for sentiment analysis. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "yacobsF7mnMI"
      },
      "outputs": [],
      "source": [
        "PRE_TRAINED_MODEL_NAME = \"cardiffnlp/twitter-roberta-base-sentiment\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DtsCWzHO7J0X"
      },
      "outputs": [],
      "source": [
        "model = SentenceTransformer(PRE_TRAINED_MODEL_NAME)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "16imGSSUmqMz"
      },
      "source": [
        "Suppose that we have a list of sentences with different sentiments:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "dcTyZ2uAnPs6"
      },
      "outputs": [],
      "source": [
        "sentences = [\n",
        "    \"I love you\",\n",
        "    \"I don't like you\",\n",
        "    \"I know you\",\n",
        "    \"I like you before and although you did something good to me, I hate you\",\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2fh2B21nQGL"
      },
      "source": [
        "Our task is that, given a query sentence, rank the sentences based on sentiment similarity:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZjFnO66gfojq"
      },
      "outputs": [],
      "source": [
        "query = \"I like you\"  # Query sentence\n",
        "\n",
        "emb = model.encode([query] + sentences)\n",
        "sim = cosine_similarity_matrix(emb)[0, 1:]\n",
        "hits = np.argsort(-sim)\n",
        "for i in hits:\n",
        "    print(\"sim = {sim:.2f}: '{sent}'\".format(sent=sentences[i], sim=sim[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZSvI2Kfn9H1"
      },
      "source": [
        "## **1.5. Interactive Hands-On**\n",
        "- With fine-tuned models\n",
        "  1. Go to the [sentence-transformers library](https://www.sbert.net/docs/pretrained_models.html) and find \"Model Overview\" section\n",
        "  2. Copy a model name into `MODEL_NAME`.\n",
        "  3. Adapt the text and make your first semantic search.\n",
        "- With pre-trained models \n",
        "  1. Go to the [Hugging Face model hub](https://huggingface.co/models) and click a model for text classification. \n",
        "  2. In the model card of a model, copy the model name at the top left and past it into `MODEL_NAME`.\n",
        "  3. Adapt the text and make your first semantic search."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "RCQNWPzOpNUw"
      },
      "outputs": [],
      "source": [
        "MODEL_NAME = \"\"\n",
        "\n",
        "model = SentenceTransformer(MODEL_NAME)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qC1RKHQFmora"
      },
      "outputs": [],
      "source": [
        "sentences = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CaeBiXxppJiw"
      },
      "outputs": [],
      "source": [
        "query = \"\"  # Query sentence\n",
        "\n",
        "emb = model.encode([query] + sentences)\n",
        "sim = cosine_similarity_matrix(emb)[0, 1:]\n",
        "hits = np.argsort(-sim)\n",
        "for i in hits:\n",
        "    print(\"sim = {sim:.2f}: '{sent}'\".format(sent=sentences[i], sim=sim[i]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CgHuFFmGqOHY"
      },
      "source": [
        "# **2. Fine-tuning models with your data**\n",
        "\n",
        "So far, we have used existing fine-tuned or pre-trained models trained with generic texts. However, we all have different problems, and thus often want to tailor the models with our data. In the following, we will walk through how to fine-tune transformer-based models using sentence-transformer architecture. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onMUSzGJr4uH"
      },
      "source": [
        "## **2.1 Setup**\n",
        "\n",
        "To start, we need to import some libraries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkGaqtHsr7lG"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import (\n",
        "    InputExample,\n",
        "    SentencesDataset,\n",
        "    SentenceTransformer,\n",
        "    evaluation,\n",
        "    losses,\n",
        "    models,\n",
        ")\n",
        "from torch.utils.data import DataLoader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ByIkIAgusRkG"
      },
      "source": [
        "## **2.2 Define a model**\n",
        "\n",
        "`sentence-transformers` library provides building blocks to define a model for sentence-transformers. Here, we construct a sentence-transformer model with a pre-trained model, `distilroberta-base`, and average pooling layer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_xB78JbpsRDi"
      },
      "outputs": [],
      "source": [
        "# Define the base model for word embeddings\n",
        "word_embedding_model = models.Transformer(\"distilroberta-base\", max_seq_length=512)\n",
        "\n",
        "# Define the pooling layer that aggregates word embeddings into a sentence embedding\n",
        "pooling_model = models.Pooling(\n",
        "    word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\"\n",
        ")\n",
        "\n",
        "# Construct a sentence transformer\n",
        "model = SentenceTransformer(\n",
        "    modules=[word_embedding_model, pooling_model],\n",
        "    #   device=0,  # Set GPU device id if GPU is available\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zx8bEGBCuBNE"
      },
      "source": [
        "##### **The followings are alternative model designs:**\n",
        "\n",
        "Model that produces unit-norm sentence embeddings:\n",
        "```python\n",
        "word_embedding_model = models.Transformer(\"distilroberta-base\", max_seq_length=512)\n",
        "pooling_model = models.Pooling(\n",
        "    word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\"\n",
        ")\n",
        "\n",
        "# This ensures the unit norm of the sentence embedding. \n",
        "normalize_model = models.Normalize()\n",
        "\n",
        "model = SentenceTransformer(\n",
        "    modules=[word_embedding_model, pooling_model, normalize_model],\n",
        "    device=-1\n",
        ")\n",
        "```\n",
        "\n",
        "Model that produces more compact sentence embeddings:\n",
        "```python\n",
        "word_embedding_model = models.Transformer(\"distilroberta-base\", max_seq_length=512)\n",
        "\n",
        "pooling_model = models.Pooling(\n",
        "    word_embedding_model.get_word_embedding_dimension(), pooling_mode=\"mean\"\n",
        ")\n",
        "\n",
        "# This reduces 768 dimensional embeddings to 256 dimensional embeddings.\n",
        "dense_model = models.Dense(\n",
        "    in_features=pooling_model.get_sentence_embedding_dimension(),\n",
        "    out_features=256,\n",
        "    activation_function=nn.Tanh(),\n",
        ")\n",
        "\n",
        "model = SentenceTransformer(\n",
        "    modules=[word_embedding_model, pooling_model, dense_model], device=-1\n",
        ")\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZY2OixvTvoRg"
      },
      "source": [
        "## **2.3 Training Data**\n",
        "\n",
        "\n",
        "To train the sentence-transformer, we need pairs of sentences that are semantically similar. The sentence pair should be wrapped with `InputExample`, and all pairs should be stored in `DataLoader`. For example, \n",
        "\n",
        "```python\n",
        "train_examples = [\n",
        "    InputExample(texts=[\"My first sentence\", \"My second sentence\"]),\n",
        "    InputExample(texts=[\"Another pair\", \"Related sentence\"]),\n",
        "]\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)\n",
        "```\n",
        "\n",
        "As a toy example, here we use the title of Physics papers. This dataset consists of 5000 pairs of papers published from the American Physical Society journals. A paper `i` is paired with another paper `j` when `i` cites `j`. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kpurmm2KuL9u"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "data_table = pd.read_csv(\n",
        "    \"https://raw.githubusercontent.com/skojaku/Practical-Guide-to-Sentence-Transformers/main/data/training-data.csv\"\n",
        ")\n",
        "data_table.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLhMWDgS1w0i"
      },
      "outputs": [],
      "source": [
        "train_examples = [\n",
        "    InputExample(texts=[row[\"src\"], row[\"trg\"]]) for _, row in data_table.iterrows()\n",
        "]\n",
        "train_dataloader = DataLoader(train_examples, shuffle=True, batch_size=16)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BEpL-LWNwKyP"
      },
      "source": [
        "## **2.4 Loss function**\n",
        "\n",
        "The loss function is by far the most critical for performance. There are several loss functions available in [sentence-transformers library](https://www.sbert.net/docs/package_reference/losses.html?highlight=loss%20functions). A common choice is *triplet loss*. See [this paper](https://arxiv.org/abs/1703.07737) for details.  \n",
        "\n",
        "Another important variable is the type of similarity for embeddings. Euclidean, dot-product, and cosine similarity are the commonly used metric for similarity. Here we use cosine similarity as a metric for similarity, which can be specified through `distance_metric` argument of the loss function. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f80ff0KIxIrj"
      },
      "outputs": [],
      "source": [
        "train_loss = losses.BatchSemiHardTripletLoss(\n",
        "    model=model,\n",
        "    distance_metric=losses.BatchHardTripletLossDistanceFunction.cosine_distance,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uUnJgUEEyD_O"
      },
      "source": [
        "## **2.5 Evaluator**\n",
        "\n",
        "The training usually takes some time, and we might want to monitor the learning progress. `evaluation` module contains various evaluators that measure the performance improvements during the training phase. See [here](https://www.sbert.net/docs/package_reference/evaluation.html?highlight=evaluators) for the available evaluators. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzD_ZKzpyDg0"
      },
      "outputs": [],
      "source": [
        "# We will make two groups of sentece pairs, `pos_pairs` and `neg_pairs`.\n",
        "# `pos_pairs` is composed of sentences paired by citatons.\n",
        "pos_pairs = data_table.sample(frac=0.1)\n",
        "pos_pairs[\"score\"] = 1  # group label\n",
        "\n",
        "# `neg_pairs` is composed of the pairs of randomly selected sentences.\n",
        "neg_pairs = data_table.copy()\n",
        "neg_pairs[\"trg\"] = neg_pairs[\"trg\"].sample(frac=1).values\n",
        "neg_pairs = neg_pairs.sample(frac=0.1)\n",
        "neg_pairs[\"score\"] = 0  # group label\n",
        "\n",
        "# Concatenate the pairs\n",
        "eval_data_table = pd.concat([pos_pairs, neg_pairs])\n",
        "\n",
        "# Set up the evaluator\n",
        "evaluator = evaluation.EmbeddingSimilarityEvaluator(\n",
        "    eval_data_table[\"src\"].values.tolist(),  # sentence\n",
        "    eval_data_table[\"trg\"].values.tolist(),  # sentence\n",
        "    scores=eval_data_table[\"score\"].values.tolist(),  # similarity\n",
        "    show_progress_bar=True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjvEgdi3zkO0"
      },
      "source": [
        "## **2.6. Training**\n",
        "\n",
        "Set some parameters for training:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NRPi89-Jzj0_"
      },
      "outputs": [],
      "source": [
        "num_epochs = 4\n",
        "warmup_steps = 100\n",
        "evaluation_steps = 1000\n",
        "model_save_path = \"model\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4an59GHszYVo"
      },
      "source": [
        "All set. We can train the model by"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4oCQr-Pzgvu"
      },
      "outputs": [],
      "source": [
        "model.fit(\n",
        "    train_objectives=[(train_dataloader, train_loss)],\n",
        "    evaluator=evaluator,\n",
        "    epochs=num_epochs,\n",
        "    evaluation_steps=evaluation_steps,\n",
        "    warmup_steps=warmup_steps,\n",
        "    output_path=model_save_path,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 3. Pipeline\n",
        "\n",
        "[pipiline](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines) is a module in transformer library that provides a convenient way to use transformer-based models for various NLP tasks. Although not directly related to sentence transformers, given its popularity and ease of use, we will briefly introduce it here.\n",
        "\n",
        "An example is worth a thousand words. Let's use it for sentiment analysis.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the sentiment analysis pipeline\n",
        "model = pipeline(\"sentiment-analysis\")\n",
        "\n",
        "# Analyze some text examples\n",
        "model(\"I love this product! It works perfectly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "`pipeline` bundles the model, tokenizer, and task-specific processing steps into a single object, so that you can use it as a function. We can specificy the model for the task by `model` argument. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Initialize the sentiment analysis pipeline\n",
        "model = pipeline(\"sentiment-analysis\", model = \"tabularisai/multilingual-sentiment-analysis\")\n",
        "\n",
        "# Analyze some text examples\n",
        "model(\"I love this product! It works perfectly.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "There are various tasks that can be performed with `pipeline`.\n",
        "- \"audio-classification\"\n",
        "- \"automatic-speech-recognition\"\n",
        "- \"depth-estimation\"\n",
        "- \"document-question-answering\"\n",
        "- \"feature-extraction\"\n",
        "- \"fill-mask\"\n",
        "- \"image-classification\"\n",
        "- \"image-feature-extraction\"\n",
        "- \"image-segmentation\"\n",
        "- \"image-text-to-text\"\n",
        "- \"image-to-image\"\n",
        "- \"image-to-text\"\n",
        "- \"mask-generation\"\n",
        "- \"object-detection\"\n",
        "- \"question-answering\"\n",
        "- \"summarization\"\n",
        "- \"table-question-answering\"\n",
        "- \"text2text-generation\"\n",
        "- \"text-classification\" (alias \"sentiment-analysis\" available)\n",
        "- \"text-generation\"\n",
        "- \"text-to-audio\" (alias \"text-to-speech\" available)\n",
        "- \"token-classification\" (alias \"ner\" available)\n",
        "- \"translation\"\n",
        "- \"translation_xx_to_yy\"\n",
        "- \"video-classification\"\n",
        "- \"visual-question-answering\"\n",
        "- \"zero-shot-classification\"\n",
        "- \"zero-shot-image-classification\"\n",
        "- \"zero-shot-audio-classification\"\n",
        "- \"zero-shot-object-detection\"\n",
        " "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# For RoBERTa model\n",
        "roberta_fill_mask = pipeline(\"fill-mask\", model=\"roberta-base\")\n",
        "\n",
        "# Example with RoBERTa mask token\n",
        "roberta_fill_mask(\"The capital of France is <mask>.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Exercise: Pipeline Exploration\n",
        "\n",
        "Now that you've seen basic examples of the pipeline module, it's time to experiment with it yourself. The pipeline abstraction makes it incredibly easy to try different NLP tasks and models without writing extensive code.\n",
        "\n",
        "Explore at least three of the following pipeline tasks:\n",
        "\n",
        "1. **Text Generation**: Generate text completions from prompts\n",
        "    ```python\n",
        "    generator = pipeline(\"text-generation\")\n",
        "    generator(\"In the distant future, artificial intelligence\")\n",
        "    ```\n",
        "\n",
        "2. **Question Answering**: Answer questions based on context\n",
        "    ```python\n",
        "    qa = pipeline(\"question-answering\")\n",
        "    qa(question=\"Where do I live?\", context=\"My name is Sarah and I live in London\")\n",
        "    ```\n",
        "\n",
        "3. **Text Summarization**: Create concise summaries of longer texts\n",
        "    ```python\n",
        "    summarizer = pipeline(\"summarization\")\n",
        "    summarizer(\"Your long article or text goes here...\", max_length=100, min_length=30)\n",
        "    ```\n",
        "\n",
        "4. **Translation**: Translate text between languages\n",
        "    ```python\n",
        "    translator = pipeline(\"translation_en_to_fr\")\n",
        "    translator(\"The house is wonderful.\")\n",
        "    ```\n",
        "\n",
        "5. **Named Entity Recognition**: Identify entities like people, organizations, locations\n",
        "    ```python\n",
        "    ner = pipeline(\"ner\")\n",
        "    ner(\"My name is Sarah Jessica Parker and I live in New York City\")\n",
        "    ```\n",
        "\n",
        "6. **Zero-shot Classification**: Classify text without pre-defined labels\n",
        "    ```python\n",
        "    classifier = pipeline(\"zero-shot-classification\")\n",
        "    classifier(\"I have a problem with my iPhone that needs to be fixed.\",\n",
        "               candidate_labels=[\"electronics\", \"travel\", \"cooking\"])\n",
        "    ```\n",
        "\n",
        "Try out different tasks and models to see how they work! "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyP3tycHL9xgaGllCelwEcir",
      "collapsed_sections": [],
      "include_colab_link": true,
      "name": "Practical Guide to Sentence Transformers",
      "private_outputs": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
